{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (5.2.0)/charset_normalizer (3.3.2) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import plotly.express as px\n",
    "from coins import get_top_coins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am opening a JSON file located at './Telegram_Data/result.json' and loading its contents using the JSON module. Then, I am iterating through the keys in the JSON data and printing each key to the console. This code allows me to inspect the keys in the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./Telegram_Data/result.json', encoding='utf8')\n",
    "data = json.load(f)\n",
    "for key in data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code cell, I am creating a Pandas DataFrame called 'df' by extracting data from the 'messages' key within the previously loaded JSON data. I limit the DataFrame to the first 2000 rows using the .iloc method. Then, I use the info() method to display information about the DataFrame, such as the data types and non-null counts of each column. Finally, I use the head() method to display the first few rows of the DataFrame for a quick overview of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['messages'])\n",
    "df = df.iloc[0:10000]\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm setting up a language detection component in a spaCy language model. This component allows the model to detect the language of text. It involves defining a function for the language detector, loading a spaCy model for English, and then adding the language detector to the spaCy pipeline. en_core_web_sm model in spacy is a pipe in which many preprocesses are applied (Tokenization, Part-of-Speech Tagging, Dependency Parsing, Named Entity Recognition (NER), Lemmatization, Stop Word Removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "if not \"language_detector\" in Language.factories:\n",
    "    Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "\n",
    "nlp.add_pipe('language_detector', last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define a function called extract_features that processes text using spaCy, extracts the detected language, and identifies named entities.\n",
    "I then apply this function to each text entry in the DataFrame df using a list comprehension and the tqdm library.\n",
    "The results are stored in the 'language' and 'entities' and 'lemmatized_text' columns of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(x):\n",
    "    doc = nlp(x)\n",
    "    lang_dict = doc._.language\n",
    "    language = lang_dict['language']\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return language, entities, lemmatized_text\n",
    "\n",
    "df['language'], df['entities'], df['lemmatized_text'] = zip(*[extract_features(str(x)) for x in tqdm(df['text'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['text', 'language','entities','lemmatized_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.language.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I'm using a custom function, get_top_coins, to retrieve information about the top 20 coins from the Coinranking API. Then, I'm counting how many times the names and symbols of these coins appear in a text dataset. The code identifies the top 5 coins with the most text mentions and presents them in a DataFrame for further analysis. The purpose is to find the most discussed coins in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_coins = get_top_coins(20)\n",
    "\n",
    "related_counts = {}\n",
    "\n",
    "for coin in top_coins:\n",
    "    name_matches = df_new['text'].str.lower().str.contains(coin['name'].lower()).sum()\n",
    "    symbol_matches = df_new['text'].str.lower().str.contains(coin['symbol'].lower()).sum()\n",
    "    total_matches = name_matches + symbol_matches\n",
    "    related_counts[coin['name']] = total_matches\n",
    "\n",
    "top_5_coins = dict(sorted(related_counts.items(), key=lambda item: item[1], reverse=True)[:5])\n",
    "\n",
    "top_5_df = pd.DataFrame(top_5_coins.items(), columns=['Coin Name', 'Related Text Count'])\n",
    "\n",
    "print(top_5_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter and prepare a dataset for analysis. It starts by selecting English-language text entries and then narrows it down further to include only rows where the text mentions specific cryptocurrencies (Bitcoin, Dogecoin, or BTC which are the tope coins in the above code) and has a minimum length. This filtered dataset is then ready for further analysis, focusing on discussions related to these cryptocurrencies in English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df[df.language.values == 'en']\n",
    "df_new = df_eng.filter(items = ['id','text','lemmatized_text', 'date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coin_sentiment(cleaned_data,from_date, to_date, coin_name,coin_symbol):\n",
    "    def Vader_senti(x):\n",
    "        \"\"\"\n",
    "        Function to calculate the sentiment of the message x.\n",
    "        Returns the probability of a given input sentence to be Negative, Neutral, Positive and Compound score.\n",
    "        \n",
    "        \"\"\"\n",
    "        scores = sid_obj.polarity_scores(x)\n",
    "        return scores['neg'],scores['neu'],scores['pos'],scores['compound']\n",
    "    \n",
    "    data = cleaned_data[cleaned_data['text'].str.lower().str.contains(coin_name|coin_symbol) & (cleaned_data['text'].str.len() > 1)]\n",
    "    data = data[(data['date'] >= from_date) & (data['date'] <= to_date)]\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    df_new[['vader_neg','vader_neu','vader_pos','vader_compound']] = [Vader_senti(x) for x in tqdm(df_new['text'])]\n",
    "    \n",
    "    def parse_date(x):\n",
    "        date_time_obj = datetime.datetime.strptime(x, '%Y-%m-%dT%H:%M:%S')\n",
    "        return date_time_obj.date()\n",
    "\n",
    "    \n",
    "    df_new['Day'] = [parse_date(x) for x in tqdm(df_new['date'])]\n",
    "    avg_sent = df_new.groupby(['Day']).agg({'vader_compound' : ['mean', 'count']})\n",
    "    avg_sent.columns = ['_'.join(str(i) for i in col) for col in avg_sent.columns]\n",
    "    avg_sent.reset_index(inplace=True)\n",
    "    avg_sent = avg_sent.rename(columns={'vader_compound_mean': f'{coin_name}_mean', 'vader_compound_count': f'{coin_name}_count'})\n",
    "    return avg_sent\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new[df_new['text'].str.lower().str.contains('bitcoin|doge|btc') & (df_new['text'].str.len() > 1)]\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function, Vader_senti, that calculates sentiment scores (Negative, Neutral, Positive, and Compound) for text messages using the VADER sentiment analysis tool. Then, it applies this function to each text entry in the 'text' column of the DataFrame df_new and stores the sentiment scores in new columns ('vader_neg', 'vader_neu', 'vader_pos', 'vader_compound') in the same DataFrame. This allows for sentiment analysis of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_obj = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23524/2893587805.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'neg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'neu'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pos'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compound'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vader_neg'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'vader_neu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'vader_pos'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'vader_compound'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mVader_senti\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vader_neg_lemma'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'vader_neu_lemma'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'vader_pos_lemma'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'vader_compound_lemma'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mVader_senti\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemmatized_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "def Vader_senti(x):\n",
    "    \"\"\"\n",
    "    Function to calculate the sentiment of the message x.\n",
    "    Returns the probability of a given input sentence to be Negative, Neutral, Positive and Compound score.\n",
    "    \n",
    "    \"\"\"\n",
    "    scores = sid_obj.polarity_scores(x)\n",
    "    return scores['neg'],scores['neu'],scores['pos'],scores['compound']\n",
    "\n",
    "df_new[['vader_neg','vader_neu','vader_pos','vader_compound']] = [Vader_senti(x) for x in tqdm(df_new['text'])]\n",
    "df_new[['vader_neg_lemma','vader_neu_lemma','vader_pos_lemma','vader_compound_lemma']] = [Vader_senti(x) for x in tqdm(df_new['lemmatized_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract date from the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(x):\n",
    "    date_time_obj = datetime.datetime.strptime(x, '%Y-%m-%dT%H:%M:%S')\n",
    "    return date_time_obj.date()\n",
    "\n",
    "df_new['Day'] = [parse_date(x) for x in tqdm(df_new['date'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code performs the following:\n",
    "\n",
    "It filters rows in the DataFrame df_new to create two new DataFrames:\n",
    "\n",
    "df_btc containing rows that mention 'btc' or 'bitcoin.'\n",
    "df_doge containing rows that mention 'doge.'\n",
    "It calculates the average sentiment score for the 'vader_compound' column in the df_doge and df_btc DataFrames, grouped by the 'Day' column. It also counts the number of entries in each group.\n",
    "\n",
    "The code then renames the columns for clarity and prints the results in two separate tables for 'doge' and 'btc' mentions, showing the average sentiment and the count of mentions for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows containing 'bitcoin'\n",
    "df_btc = df_new[df_new['text'].str.lower().str.contains('btc|bitcoin')]\n",
    "\n",
    "# Filter rows containing 'doge'\n",
    "df_doge = df_new[df_new['text'].str.lower().str.contains('doge')]\n",
    "avg_sent = df_doge.groupby(['Day']).agg({'vader_compound' : ['mean', 'count']})\n",
    "avg_sent.columns = ['_'.join(str(i) for i in col) for col in avg_sent.columns]\n",
    "avg_sent.reset_index(inplace=True)\n",
    "avg_sent = avg_sent.rename(columns={'vader_compound_mean': 'doge_mean', 'vader_compound_count': 'doge_count'})\n",
    "print(avg_sent)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "avg_sent2 = df_btc.groupby(['Day']).agg({'vader_compound' : ['mean', 'count']})\n",
    "avg_sent2.columns = ['_'.join(str(i) for i in col) for col in avg_sent2.columns]\n",
    "avg_sent2.reset_index(inplace=True)\n",
    "avg_sent2 = avg_sent2.rename(columns={'vader_compound_mean': 'btc_mean', 'vader_compound_count': 'btc_count'})\n",
    "print(avg_sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the exact same thing for lemmatized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows containing 'bitcoin'\n",
    "df_btc = df_new[df_new['lemmatized_text'].str.lower().str.contains('btc|bitcoin')]\n",
    "\n",
    "# Filter rows containing 'doge'\n",
    "df_doge = df_new[df_new['lemmatized_text'].str.lower().str.contains('doge')]\n",
    "avg_sent_lemma = df_doge.groupby(['Day']).agg({'vader_compound_lemma' : ['mean', 'count']})\n",
    "avg_sent_lemma.columns = ['_'.join(str(i) for i in col) for col in avg_sent_lemma.columns]\n",
    "avg_sent_lemma.reset_index(inplace=True)\n",
    "avg_sent_lemma = avg_sent_lemma.rename(columns={'vader_compound_lemma_mean': 'doge_mean', 'vader_compound_lemma_count': 'doge_count'})\n",
    "print(avg_sent_lemma)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "avg_sent2_lemma = df_btc.groupby(['Day']).agg({'vader_compound_lemma' : ['mean', 'count']})\n",
    "avg_sent2_lemma.columns = ['_'.join(str(i) for i in col) for col in avg_sent2_lemma.columns]\n",
    "avg_sent2_lemma.reset_index(inplace=True)\n",
    "avg_sent2_lemma = avg_sent2_lemma.rename(columns={'vader_compound_lemma_mean': 'btc_mean', 'vader_compound_lemma_count': 'btc_count'})\n",
    "print(avg_sent2_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm adding a 'Sentiment' column to the DataFrames avg_sent and avg_sent2. The 'Sentiment' column classifies each day's sentiment as \"Negative\" if the average sentiment score is less than 0 and as \"Positive\" if it's greater than or equal to 0. This categorizes sentiment results into two simple categories: \"Negative\" and \"Positive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sent['Sentiment'] = [\"Negative\" if x<0 else \"Positive\"\n",
    "                         for x in avg_sent['doge_mean']]\n",
    "avg_sent2['Sentiment'] = [\"Negative\" if x<0 else \"Positive\"\n",
    "                         for x in avg_sent2['btc_mean']]\n",
    "\n",
    "avg_sent_lemma['Sentiment'] = [\"Negative\" if x<0 else \"Positive\"\n",
    "                         for x in avg_sent_lemma['doge_mean']]\n",
    "avg_sent2_lemma['Sentiment'] = [\"Negative\" if x<0 else \"Positive\"\n",
    "                         for x in avg_sent2_lemma['btc_mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an output for showing the message count related to each coin in each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.add_trace(px.histogram(avg_sent, x=\"Day\", y=\"doge_count\", color=\"Sentiment\", nbins=20).data[0], row=1, col=1)\n",
    "\n",
    "fig.add_trace(px.histogram(avg_sent2, x=\"Day\", y=\"btc_count\", color=\"Sentiment\", nbins=20).data[0], row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Vader Sentiment Analysis Results',\n",
    "    bargap=0.2, \n",
    "    bargroupgap=0.1 \n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Doge Coin\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Bitcoin\", row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for lemmatized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.add_trace(px.histogram(avg_sent_lemma, x=\"Day\", y=\"doge_count\", color=\"Sentiment\", nbins=20).data[0], row=1, col=1)\n",
    "\n",
    "fig.add_trace(px.histogram(avg_sent2_lemma, x=\"Day\", y=\"btc_count\", color=\"Sentiment\", nbins=20).data[0], row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Vader Sentiment Analysis Results',\n",
    "    bargap=0.2, \n",
    "    bargroupgap=0.1 \n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Doge Coin\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Bitcoin\", row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overall, lemmatized text act better and more logical in sentiment vader analysis, however the charts illustrate a roughly similar figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
